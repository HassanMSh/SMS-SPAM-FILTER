# -*- coding: utf-8 -*-
"""Process-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rmKmYBghtE7i5u0facJCUx6W8mta2K7c
"""

# Import necessary libraries

from operator import index

import pandas as pd

!pip install langdetect
import langdetect as detect

import numpy as np

import os

import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer

import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

from google.colab import drive

# Download necessary data for natural language processing tasks

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')

# Mount Google Drive to access data

drive.mount('/content/drive/')

# Read in spam data from CSV file

df = pd.read_csv('/content/drive/MyDrive/Training/spam.csv',
                 sep=',', header=0, on_bad_lines='skip', encoding = "ISO-8859-1")

# Drop any "Unnamed" columns

unnamed_cols = df.columns[df.columns.str.contains("Unnamed")]
df.drop(columns=unnamed_cols, inplace=True)

# Define regular expression variables to remove from SMS messages

reg_vars = ['http\S+', 'www\S+', 'https\S+', '\W\s+', '\d+', '\t+', '\d+', '\-+', '\\+', '\/+', '\"+', '\#+', '\++', '\@+', '\$+', '\%+', '\^+', '\&+', '\*+', '\(+', '\)+', '\[+', '\]+', '\{+', '\}+', '\|+', '\;+', '\:+', '\<+', '\>+', '\?+', '\,+', '\.+', '\=+', '\_+', '\~+', '\`+', '\s+']

df.replace(reg_vars, ' ', regex=True, inplace=True)

df.drop_duplicates(inplace=True)

df.replace('', np.nan, inplace=True)

df.dropna(inplace=True)

# Remove rows with non-ASCII characters from the dataframe

df = df[df['v2'].map(lambda x: x.isascii())]

# Drop non-English rows from the dataframe

for i in range(len(df)):
    try:
        ['v2'][i] = detect.detect(df['v2'][i])
        if df['v2'][i] != 'en':
            df.drop(i, inplace=True, index=False)
    except:
        pass

# Convert all the text data into lowercase

df['v2'] = df['v2'].astype(str).str.lower()

# Retrieve a list of English stop words and assign it to a var

stopwords = nltk.corpus.stopwords.words("english")

# Tokenize the SMS messages in the dataframe

df['TokenSMS'] = df.apply(lambda column: nltk.word_tokenize(column['v2']), axis=1)

df['TokenSMS'].head(2)

# Create a column containing the Tokenized words without the stopwords

df['StopTokenSMS'] = df['TokenSMS'].apply(lambda x: [item for item in x if item not in stopwords])

# Create a coolumn containting the StopTokenSMS text with words less than 2 characters

df['LengthTokenSMS'] = df['StopTokenSMS'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))

# Instantiate and the assigne a variable to the WordNetLemmatizer class

wordnet_lem = WordNetLemmatizer()

# Create a new column which contains the lemmatized words

df['LemTokenSMS'] = df['LengthTokenSMS'].apply(wordnet_lem.lemmatize)

# Clean all dataframes again

reg_vars = ['http\S+', 'www\S+', 'https\S+', '\W\s+', '\d+', '\t+', '\d+', '\-+', '\\+', '\/+', '\"+', '\#+', '\++', '\@+', '\$+', '\%+', '\^+', '\&+', '\*+', '\(+', '\)+', '\[+', '\]+', '\{+', '\}+', '\|+', '\;+', '\:+', '\<+', '\>+', '\?+', '\,+', '\.+', '\=+', '\_+', '\~+', '\`+', '\s+']

df.replace(reg_vars, ' ', regex=True, inplace=True)

df.replace('', np.nan, inplace=True)

df.dropna(inplace=True)

# Initialize a CountVectorizer object

cv = CountVectorizer()

# fit_transform the data to a numpy array

x = cv.fit_transform(df['LemTokenSMS']).toarray()

x.shape

# Replace 'spam' with 1 and 'ham' with 0
df['v1'] = df['v1'].replace({'spam': 1, 'ham': 0})

# Store the labels in  y

y = df['v1'].values

y.shape

# convert y to to int type

y = y.astype('int')

# Split the data into a training set and testing set

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

x_train

y_train.astype

# Initialize a MultinomialNB objec

mnb=MultinomialNB()

# Training the classifier and making predictions on the test data

mnb.fit(x_train,y_train)
y_pred=mnb.predict(x_test)
print(accuracy_score(y_test,y_pred))

import pickle

# Save the model to a file
with open('model.pkl', 'wb') as file:
    pickle.dump(mnb, file)

# Save the model to a file
with open('cv.pkl', 'wb') as file:
    pickle.dump(cv, file)

from google.colab import files

files.download('model.pkl')
files.download('cv.pkl')

# Create a preprocessing function to process new text

def clean_dataframe(df):
    reg_vars = ['http\S+', 'www\S+', 'https\S+', '\W\s+', '\d+', '\t+', '\d+', '\-+', '\\+', '\/+', '\"+', '\#+', '\++', '\@+', '\$+', '\%+', '\^+', '\&+', '\*+', '\(+', '\)+', '\[+', '\]+', '\{+', '\}+', '\|+', '\;+', '\:+', '\<+', '\>+', '\?+', '\,+', '\.+', '\=+', '\_+', '\~+', '\`+', '\s+']
    df['text'].replace(reg_vars, ' ', regex=True, inplace=True)
    df['text'] = df['text'].astype(str).str.lower()
    df['text'] = df.apply(lambda column: nltk.word_tokenize(column['text']), axis=1)
    stopwords = nltk.corpus.stopwords.words('english')
    df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stopwords])
    df['text'] = df['text'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))
    df['text'] = df['text'].apply(wordnet_lem.lemmatize)

data = [{"text": "Urgent dont miss news dun say so early hor... U c already then say lucky man"}]

ndf = pd.DataFrame(data)

clean_dataframe(ndf)

newtext = cv.transform(ndf['text']).toarray()
prediction = mnb.predict(newtext)
print(prediction)